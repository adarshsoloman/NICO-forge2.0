{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üöÄ Bilingual JSONL Dataset Processing Pipeline\n",
                "\n",
                "This notebook processes English-Hindi bilingual datasets in JSONL format.\n",
                "\n",
                "## üìã Pipeline Steps:\n",
                "1. **Basic Cleaning** - Remove newlines, normalize whitespace\n",
                "2. **LLM Deep Cleaning** - AI-powered verification and cleaning\n",
                "3. **Phase 1 Chunking** - Split into 3-sentence chunks\n",
                "4. **Phase 2 Chunking** - LLM-assisted alignment for mismatches\n",
                "5. **Merge Results** - Combine into final dataset\n",
                "\n",
                "## üîß Setup Instructions:\n",
                "1. Upload your `pib_bilingual.jsonl` file (raw data from Rewat)\n",
                "2. Set your OpenRouter API key in Step 0\n",
                "3. Run cells in order (just click \"Run All\"!)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 0: Configuration & Setup\n",
                "\n",
                "Set your LLM API credentials here:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CONFIGURATION - UPDATE THESE VALUES\n",
                "# ============================================================================\n",
                "\n",
                "# LLM Configuration (for Steps 2 and 4)\n",
                "LLM_API_KEY = \"your-openrouter-api-key-here\"  # Get from https://openrouter.ai/\n",
                "LLM_BASE_URL = \"https://openrouter.ai/api/v1\"\n",
                "LLM_MODEL = \"meta-llama/llama-3.1-8b-instruct:free\"  # Free tier model\n",
                "\n",
                "# File names (don't change unless you renamed your files)\n",
                "INPUT_FILE = \"pib_bilingual.jsonl\"  # Your uploaded raw file\n",
                "FINAL_OUTPUT = \"pib_final_chunked_dataset.jsonl\"  # Final result\n",
                "\n",
                "print(\"‚úÖ Configuration loaded!\")\n",
                "print(f\"   Model: {LLM_MODEL}\")\n",
                "print(f\"   Input: {INPUT_FILE}\")\n",
                "print(f\"   Final Output: {FINAL_OUTPUT}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages\n",
                "!pip install -q openai\n",
                "\n",
                "import json\n",
                "import re\n",
                "import time\n",
                "from openai import OpenAI\n",
                "\n",
                "print(\"‚úÖ All dependencies installed!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Upload Your JSONL File\n",
                "\n",
                "Click the folder icon on the left sidebar and upload your `pib_bilingual.jsonl` file."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "# Check if input file exists\n",
                "if os.path.exists(INPUT_FILE):\n",
                "    file_size = os.path.getsize(INPUT_FILE)\n",
                "    with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
                "        num_lines = sum(1 for _ in f)\n",
                "    print(f\"‚úÖ Input file found!\")\n",
                "    print(f\"   File: {INPUT_FILE}\")\n",
                "    print(f\"   Size: {file_size:,} bytes\")\n",
                "    print(f\"   Entries: {num_lines}\")\n",
                "else:\n",
                "    print(f\"‚ùå ERROR: {INPUT_FILE} not found!\")\n",
                "    print(f\"   Please upload your file using the folder icon on the left.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# üßπ PHASE 1: Basic Cleaning\n",
                "\n",
                "Removes newlines, normalizes whitespace, cleans special characters."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def clean_text(text):\n",
                "    \"\"\"Clean text by removing special characters and excessive whitespace.\"\"\"\n",
                "    if not text:\n",
                "        return \"\"\n",
                "    \n",
                "    # Replace newlines with spaces\n",
                "    text = text.replace('\\\\n', ' ')\n",
                "    \n",
                "    # Replace multiple spaces with single space\n",
                "    text = re.sub(r'\\s+', ' ', text)\n",
                "    \n",
                "    # Remove leading/trailing whitespace\n",
                "    text = text.strip()\n",
                "    \n",
                "    return text\n",
                "\n",
                "\n",
                "def step1_basic_cleaning(input_file, output_file):\n",
                "    \"\"\"Step 1: Basic text cleaning.\"\"\"\n",
                "    print(\"=\" * 70)\n",
                "    print(\"STEP 1: Basic Cleaning\")\n",
                "    print(\"=\" * 70)\n",
                "    print(f\"Input:  {input_file}\")\n",
                "    print(f\"Output: {output_file}\\n\")\n",
                "    \n",
                "    processed = 0\n",
                "    \n",
                "    with open(input_file, 'r', encoding='utf-8') as infile, \\\n",
                "         open(output_file, 'w', encoding='utf-8') as outfile:\n",
                "        \n",
                "        for line_num, line in enumerate(infile, 1):\n",
                "            try:\n",
                "                data = json.loads(line)\n",
                "                \n",
                "                cleaned_entry = {\n",
                "                    'english': clean_text(data.get('english', '')),\n",
                "                    'hindi': clean_text(data.get('hindi', ''))\n",
                "                }\n",
                "                \n",
                "                outfile.write(json.dumps(cleaned_entry, ensure_ascii=False) + '\\n')\n",
                "                processed += 1\n",
                "                \n",
                "                if line_num % 50 == 0:\n",
                "                    print(f\"  Processed: {line_num} entries...\")\n",
                "                    \n",
                "            except Exception as e:\n",
                "                print(f\"‚ö† Warning: Skipping line {line_num}: {e}\")\n",
                "    \n",
                "    print(f\"\\n‚úÖ Step 1 Complete!\")\n",
                "    print(f\"   Processed: {processed} entries\")\n",
                "    print(f\"   Output: {output_file}\\n\")\n",
                "    return processed\n",
                "\n",
                "\n",
                "# RUN STEP 1\n",
                "cleaned_file = \"pib_bilingual_cleaned.jsonl\"\n",
                "step1_count = step1_basic_cleaning(INPUT_FILE, cleaned_file)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# ü§ñ PHASE 2: LLM Deep Cleaning (Optional)\n",
                "\n",
                "Uses AI to intelligently verify and deep clean the dataset.\n",
                "\n",
                "‚ö†Ô∏è **Note:** This step uses API calls and may take time. Skip if you want faster processing."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize LLM client\n",
                "client = OpenAI(\n",
                "    api_key=LLM_API_KEY,\n",
                "    base_url=LLM_BASE_URL\n",
                ")\n",
                "\n",
                "SYSTEM_PROMPT = \"\"\"You are a bilingual data quality expert specializing in English-Hindi translation pairs.\n",
                "\n",
                "Your task is to clean and verify translation pairs. Follow these rules:\n",
                "\n",
                "1. CLEANING:\n",
                "   - Remove special characters like backslashes (\\\\), forward slashes (/) that don't belong\n",
                "   - Remove escape sequences or formatting artifacts\n",
                "   - Preserve meaningful punctuation and numbers\n",
                "\n",
                "2. VERIFICATION:\n",
                "   - Check if English and Hindi are actual translations\n",
                "   - Ensure semantic alignment\n",
                "   - Do NOT retranslate or paraphrase\n",
                "\n",
                "3. OUTPUT: Return ONLY a JSON object:\n",
                "   {\"english\": \"cleaned text\", \"hindi\": \"cleaned text\", \"is_aligned\": true/false, \"issues_found\": \"description\"}\n",
                "\"\"\"\n",
                "\n",
                "\n",
                "def llm_clean_pair(english, hindi, entry_num):\n",
                "    \"\"\"Send pair to LLM for cleaning.\"\"\"\n",
                "    user_prompt = f\"\"\"Clean this English-Hindi translation pair:\n",
                "\n",
                "ENGLISH: {english}\n",
                "\n",
                "HINDI: {hindi}\n",
                "\n",
                "Return cleaned version as JSON.\"\"\"\n",
                "    \n",
                "    try:\n",
                "        response = client.chat.completions.create(\n",
                "            model=LLM_MODEL,\n",
                "            messages=[\n",
                "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
                "                {\"role\": \"user\", \"content\": user_prompt}\n",
                "            ],\n",
                "            temperature=0.1,\n",
                "            max_tokens=4000\n",
                "        )\n",
                "        \n",
                "        llm_output = response.choices[0].message.content.strip()\n",
                "        \n",
                "        # Remove markdown code blocks if present\n",
                "        if llm_output.startswith('```'):\n",
                "            llm_output = llm_output.split('```')[1]\n",
                "            if llm_output.startswith('json'):\n",
                "                llm_output = llm_output[4:]\n",
                "            llm_output = llm_output.strip()\n",
                "        \n",
                "        result = json.loads(llm_output)\n",
                "        return {\n",
                "            'english': result.get('english', english),\n",
                "            'hindi': result.get('hindi', hindi),\n",
                "            'is_aligned': result.get('is_aligned', True),\n",
                "            'verified': True\n",
                "        }\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"  ‚ö† Entry {entry_num}: LLM error, using original\")\n",
                "        return {'english': english, 'hindi': hindi, 'is_aligned': True, 'verified': False}\n",
                "\n",
                "\n",
                "def step2_llm_cleaning(input_file, output_file):\n",
                "    \"\"\"Step 2: LLM-powered deep cleaning.\"\"\"\n",
                "    print(\"=\" * 70)\n",
                "    print(\"STEP 2: LLM Deep Cleaning\")\n",
                "    print(\"=\" * 70)\n",
                "    print(f\"Model: {LLM_MODEL}\")\n",
                "    print(f\"Input:  {input_file}\")\n",
                "    print(f\"Output: {output_file}\\n\")\n",
                "    \n",
                "    processed = 0\n",
                "    verified = 0\n",
                "    \n",
                "    with open(input_file, 'r', encoding='utf-8') as infile, \\\n",
                "         open(output_file, 'w', encoding='utf-8') as outfile:\n",
                "        \n",
                "        for line_num, line in enumerate(infile, 1):\n",
                "            try:\n",
                "                data = json.loads(line)\n",
                "                print(f\"Processing entry {line_num}...\", end=\" \")\n",
                "                \n",
                "                result = llm_clean_pair(data['english'], data['hindi'], line_num)\n",
                "                \n",
                "                if result['verified']:\n",
                "                    verified += 1\n",
                "                    print(\"‚úì\")\n",
                "                else:\n",
                "                    print(\"‚ö†\")\n",
                "                \n",
                "                output_entry = {\n",
                "                    'english': result['english'],\n",
                "                    'hindi': result['hindi']\n",
                "                }\n",
                "                \n",
                "                outfile.write(json.dumps(output_entry, ensure_ascii=False) + '\\n')\n",
                "                processed += 1\n",
                "                \n",
                "                # Rate limiting\n",
                "                time.sleep(0.5)\n",
                "                \n",
                "            except Exception as e:\n",
                "                print(f\"  ‚úó Error: {e}\")\n",
                "    \n",
                "    print(f\"\\n‚úÖ Step 2 Complete!\")\n",
                "    print(f\"   Processed: {processed}\")\n",
                "    print(f\"   LLM Verified: {verified}\")\n",
                "    print(f\"   Output: {output_file}\\n\")\n",
                "    return processed\n",
                "\n",
                "\n",
                "# RUN STEP 2\n",
                "final_clean_file = \"pib_bilingual_final.jsonl\"\n",
                "step2_count = step2_llm_cleaning(cleaned_file, final_clean_file)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# ‚úÇÔ∏è PHASE 3: Sentence Chunking - Phase 1\n",
                "\n",
                "Splits long texts into 3-sentence chunks. Handles entries with matching sentence counts."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def split_english_sentences(text):\n",
                "    \"\"\"Split English text into sentences.\"\"\"\n",
                "    # Handle abbreviations\n",
                "    text = text.replace('Dr.', 'Dr<DOT>')\n",
                "    text = text.replace('Mr.', 'Mr<DOT>')\n",
                "    text = text.replace('Mrs.', 'Mrs<DOT>')\n",
                "    text = text.replace('U.S.', 'U<DOT>S<DOT>')\n",
                "    text = text.replace('etc.', 'etc<DOT>')\n",
                "    \n",
                "    # Split on sentence endings\n",
                "    sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z])', text)\n",
                "    \n",
                "    # Restore abbreviations\n",
                "    sentences = [s.replace('<DOT>', '.').strip() for s in sentences if s.strip()]\n",
                "    return sentences\n",
                "\n",
                "\n",
                "def split_hindi_sentences(text):\n",
                "    \"\"\"Split Hindi text into sentences.\"\"\"\n",
                "    sentences = re.split(r'[‡•§.!?]\\s+', text)\n",
                "    return [s.strip() for s in sentences if s.strip()]\n",
                "\n",
                "\n",
                "def chunk_into_groups(sentences, chunk_size=3):\n",
                "    \"\"\"Chunk sentences into groups.\"\"\"\n",
                "    chunks = []\n",
                "    for i in range(0, len(sentences), chunk_size):\n",
                "        chunk = sentences[i:i+chunk_size]\n",
                "        chunks.append(' '.join(chunk))\n",
                "    return chunks\n",
                "\n",
                "\n",
                "def step3_phase1_chunking(input_file, matched_file, mismatched_file):\n",
                "    \"\"\"Step 3: Phase 1 chunking.\"\"\"\n",
                "    print(\"=\" * 70)\n",
                "    print(\"STEP 3: Phase 1 Chunking\")\n",
                "    print(\"=\" * 70)\n",
                "    print(f\"Input: {input_file}\\n\")\n",
                "    \n",
                "    with open(input_file, 'r', encoding='utf-8') as f:\n",
                "        entries = [json.loads(line) for line in f]\n",
                "    \n",
                "    matched_chunks = []\n",
                "    mismatched = []\n",
                "    \n",
                "    for i, entry in enumerate(entries, 1):\n",
                "        eng_sentences = split_english_sentences(entry['english'])\n",
                "        hin_sentences = split_hindi_sentences(entry['hindi'])\n",
                "        \n",
                "        eng_count = len(eng_sentences)\n",
                "        hin_count = len(hin_sentences)\n",
                "        \n",
                "        print(f\"Entry {i}: EN={eng_count}, HI={hin_count}\", end=\" \")\n",
                "        \n",
                "        if eng_count == hin_count:\n",
                "            print(\"‚úì MATCH\")\n",
                "            \n",
                "            eng_chunks = chunk_into_groups(eng_sentences, 3)\n",
                "            hin_chunks = chunk_into_groups(hin_sentences, 3)\n",
                "            \n",
                "            for eng_chunk, hin_chunk in zip(eng_chunks, hin_chunks):\n",
                "                matched_chunks.append({\n",
                "                    'english': eng_chunk,\n",
                "                    'hindi': hin_chunk\n",
                "                })\n",
                "        else:\n",
                "            print(\"‚ö† MISMATCH\")\n",
                "            mismatched.append({\n",
                "                'entry_num': i,\n",
                "                'english': entry['english'],\n",
                "                'hindi': entry['hindi'],\n",
                "                'eng_sentences': eng_count,\n",
                "                'hin_sentences': hin_count\n",
                "            })\n",
                "    \n",
                "    # Save matched chunks\n",
                "    with open(matched_file, 'w', encoding='utf-8') as f:\n",
                "        for chunk in matched_chunks:\n",
                "            f.write(json.dumps(chunk, ensure_ascii=False) + '\\n')\n",
                "    \n",
                "    # Save mismatched for Phase 2\n",
                "    with open(mismatched_file, 'w', encoding='utf-8') as f:\n",
                "        for entry in mismatched:\n",
                "            f.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n",
                "    \n",
                "    print(f\"\\n‚úÖ Step 3 Complete!\")\n",
                "    print(f\"   Matched chunks: {len(matched_chunks)}\")\n",
                "    print(f\"   Mismatched entries: {len(mismatched)}\")\n",
                "    print(f\"   Matched output: {matched_file}\")\n",
                "    print(f\"   Mismatched output: {mismatched_file}\\n\")\n",
                "    \n",
                "    return len(matched_chunks), len(mismatched)\n",
                "\n",
                "\n",
                "# RUN STEP 3\n",
                "matched_file = \"pib_chunked_matched.jsonl\"\n",
                "mismatched_file = \"pib_mismatched_for_llm.jsonl\"\n",
                "matched_count, mismatched_count = step3_phase1_chunking(final_clean_file, matched_file, mismatched_file)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# ü§ñ PHASE 4: Sentence Chunking - Phase 2 (LLM-Assisted)\n",
                "\n",
                "Uses LLM to handle mismatched entries (different sentence counts).\n",
                "\n",
                "‚ö†Ô∏è **Note:** Only runs if there are mismatched entries from Phase 1."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def step4_phase2_chunking(input_file, output_file):\n",
                "    \"\"\"Step 4: Phase 2 LLM-assisted chunking.\"\"\"\n",
                "    print(\"=\" * 70)\n",
                "    print(\"STEP 4: Phase 2 LLM Chunking\")\n",
                "    print(\"=\" * 70)\n",
                "    \n",
                "    # Check if there are mismatched entries\n",
                "    import os\n",
                "    if not os.path.exists(input_file) or os.path.getsize(input_file) == 0:\n",
                "        print(\"\\n‚úÖ No mismatched entries! Skipping Phase 2.\\n\")\n",
                "        return 0\n",
                "    \n",
                "    print(f\"Input: {input_file}\\n\")\n",
                "    \n",
                "    system_prompt = \"\"\"You are an expert at chunking English-Hindi bilingual text.\n",
                "\n",
                "Given English and Hindi texts with different sentence counts, create aligned 3-sentence chunks.\n",
                "\n",
                "Rules:\n",
                "1. Each chunk should have 1-3 sentences\n",
                "2. English and Hindi chunks must be semantically aligned\n",
                "3. Return JSON array: [{\"english\": \"chunk\", \"hindi\": \"chunk\"}, ...]\n",
                "\"\"\"\n",
                "    \n",
                "    with open(input_file, 'r', encoding='utf-8') as f:\n",
                "        entries = [json.loads(line) for line in f]\n",
                "    \n",
                "    all_chunks = []\n",
                "    \n",
                "    for i, entry in enumerate(entries, 1):\n",
                "        try:\n",
                "            print(f\"Processing entry {i}/{len(entries)}...\", end=\" \")\n",
                "            \n",
                "            user_prompt = f\"\"\"English ({entry['eng_sentences']} sentences):\n",
                "{entry['english']}\n",
                "\n",
                "Hindi ({entry['hin_sentences']} sentences):\n",
                "{entry['hindi']}\n",
                "\n",
                "Create aligned 3-sentence chunks as JSON array.\"\"\"\n",
                "            \n",
                "            response = client.chat.completions.create(\n",
                "                model=LLM_MODEL,\n",
                "                messages=[\n",
                "                    {\"role\": \"system\", \"content\": system_prompt},\n",
                "                    {\"role\": \"user\", \"content\": user_prompt}\n",
                "                ],\n",
                "                temperature=0.2,\n",
                "                max_tokens=4000\n",
                "            )\n",
                "            \n",
                "            llm_output = response.choices[0].message.content.strip()\n",
                "            \n",
                "            # Remove code blocks\n",
                "            if llm_output.startswith('```'):\n",
                "                llm_output = llm_output.split('```')[1]\n",
                "                if llm_output.startswith('json'):\n",
                "                    llm_output = llm_output[4:]\n",
                "                llm_output = llm_output.strip()\n",
                "            \n",
                "            chunks = json.loads(llm_output)\n",
                "            all_chunks.extend(chunks)\n",
                "            print(f\"‚úì ({len(chunks)} chunks)\")\n",
                "            \n",
                "            time.sleep(0.5)\n",
                "            \n",
                "        except Exception as e:\n",
                "            print(f\"‚úó Error: {e}\")\n",
                "    \n",
                "    # Save LLM-aligned chunks\n",
                "    with open(output_file, 'w', encoding='utf-8') as f:\n",
                "        for chunk in all_chunks:\n",
                "            f.write(json.dumps(chunk, ensure_ascii=False) + '\\n')\n",
                "    \n",
                "    print(f\"\\n‚úÖ Step 4 Complete!\")\n",
                "    print(f\"   LLM-aligned chunks: {len(all_chunks)}\")\n",
                "    print(f\"   Output: {output_file}\\n\")\n",
                "    \n",
                "    return len(all_chunks)\n",
                "\n",
                "\n",
                "# RUN STEP 4\n",
                "llm_aligned_file = \"pib_chunked_llm_aligned.jsonl\"\n",
                "llm_chunks = step4_phase2_chunking(mismatched_file, llm_aligned_file)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# üéØ PHASE 5: Merge All Results\n",
                "\n",
                "Combines matched chunks and LLM-aligned chunks into final dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def step5_merge_results(matched_file, llm_file, output_file):\n",
                "    \"\"\"Step 5: Merge all chunks.\"\"\"\n",
                "    print(\"=\" * 70)\n",
                "    print(\"STEP 5: Merging Results\")\n",
                "    print(\"=\" * 70)\n",
                "    \n",
                "    total_chunks = 0\n",
                "    \n",
                "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
                "        # Add matched chunks\n",
                "        if os.path.exists(matched_file):\n",
                "            with open(matched_file, 'r', encoding='utf-8') as f:\n",
                "                for line in f:\n",
                "                    outfile.write(line)\n",
                "                    total_chunks += 1\n",
                "            print(f\"‚úì Added matched chunks from {matched_file}\")\n",
                "        \n",
                "        # Add LLM-aligned chunks\n",
                "        if os.path.exists(llm_file) and os.path.getsize(llm_file) > 0:\n",
                "            with open(llm_file, 'r', encoding='utf-8') as f:\n",
                "                for line in f:\n",
                "                    outfile.write(line)\n",
                "                    total_chunks += 1\n",
                "            print(f\"‚úì Added LLM-aligned chunks from {llm_file}\")\n",
                "    \n",
                "    print(f\"\\n‚úÖ Step 5 Complete!\")\n",
                "    print(f\"   Total chunks in final dataset: {total_chunks}\")\n",
                "    print(f\"   Final output: {output_file}\\n\")\n",
                "    \n",
                "    return total_chunks\n",
                "\n",
                "\n",
                "# RUN STEP 5\n",
                "final_chunks = step5_merge_results(matched_file, llm_aligned_file, FINAL_OUTPUT)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# ‚úÖ FINAL SUMMARY\n",
                "\n",
                "Review the complete processing pipeline results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 70)\n",
                "print(\"üéâ PIPELINE COMPLETE!\")\n",
                "print(\"=\" * 70)\n",
                "print(\"\\nProcessing Summary:\")\n",
                "print(f\"  Step 1 - Basic Cleaning:     {step1_count} entries\")\n",
                "print(f\"  Step 2 - LLM Deep Cleaning:  {step2_count} entries\")\n",
                "print(f\"  Step 3 - Phase 1 Chunking:   {matched_count} chunks\")\n",
                "print(f\"  Step 4 - Phase 2 LLM:        {llm_chunks} chunks\")\n",
                "print(f\"  Step 5 - Final Merge:        {final_chunks} chunks\")\n",
                "print(f\"\\nüìÅ Output Files:\")\n",
                "print(f\"  ‚úÖ {FINAL_OUTPUT} ({final_chunks} chunks)\")\n",
                "print(f\"\\nüí° Next Steps:\")\n",
                "print(f\"  1. Download {FINAL_OUTPUT} using the file browser\")\n",
                "print(f\"  2. Use this dataset for training your bilingual models\")\n",
                "print(f\"  3. Analyze quality and alignment\\n\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "# Show sample from final dataset\n",
                "print(\"\\nüìä Sample from Final Dataset (first 3 entries):\\n\")\n",
                "with open(FINAL_OUTPUT, 'r', encoding='utf-8') as f:\n",
                "    for i, line in enumerate(f, 1):\n",
                "        if i > 3:\n",
                "            break\n",
                "        data = json.loads(line)\n",
                "        print(f\"Entry {i}:\")\n",
                "        print(f\"  English: {data['english'][:100]}...\")\n",
                "        print(f\"  Hindi:   {data['hindi'][:100]}...\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# üì• Download Your Final Dataset\n",
                "\n",
                "Click the folder icon on the left, find `pib_final_chunked_dataset.jsonl`, and download it!"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}